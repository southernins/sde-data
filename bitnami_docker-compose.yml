## based on
## https://medium.com/@mehmood9501/using-apache-spark-docker-containers-to-run-pyspark-programs-using-spark-submit-afd6da480e0f#:~:text=Run%20PySpark%20on%20Dockerized%20Spark%20using%20spark%2Dsubmit&text=For%20that%2C%20we'll%20run,script%20using%20spark%2Dsubmit%20command.&text=This%20is%20a%20simple%20PySpark,can%20use%20docker%20cp%20command.

version: '3.7'

services:
  spark-master:
    image: bitnami/spark:latest
    command: bin/spark-class org.apache.spark.deploy.master.Master
    volumes:
      - ./apps:/opt/bitnami/spark/apps

    ports:
      - ${SPARK_PORT}:8080
      - "7077:7077"
      - "8081:8081"

  spark-worker-1:
    image: bitnami/spark:latest
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    environment:
      SPARK_MODE: worker
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 2g
      SPARK_MASTER_URL: spark://spark-master:7077

  spark-worker-2:
    image: bitnami/spark:latest
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    environment:
      SPARK_MODE: worker
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 2g
      SPARK_MASTER_URL: spark://spark-master:7077
