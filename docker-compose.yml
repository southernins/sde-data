version: "3.7"

networks:
    data-net:

services:

  spark:
    build:
      context: ./containers/spark
    container_name: ${COMPOSE_PROJECT_NAME}.spark
    entrypoint: /opt/spark/bin/spark-submit
    volumes:
      - ./apps:/opt/spark/work-dir
      - ./data:/opt/spark/data
    ports:
      - ${SPARK_PORT}:8080
    environment:
      - ENABLE_INIT_DAEMON=false
      - PYTHONPATH=/opt/spark/work-dir
    networks:
      - data-net

  pytest:
    image: ${COMPOSE_PROJECT_NAME}-spark
    container_name: ${COMPOSE_PROJECT_NAME}.test
    volumes:
      - ./apps:/opt/spark/work-dir
    depends_on:
      - spark
    entrypoint: [ 'pytest', '/opt/spark/work-dir/tests' ]
    networks:
      - data-net


  pytest-coverage:
    image: ${COMPOSE_PROJECT_NAME}-spark
    container_name: ${COMPOSE_PROJECT_NAME}.test-coverage
    volumes:
      - ./apps:/opt/spark/work-dir
    depends_on:
      - spark
    entrypoint: [ 'coverage run pytest', '/opt/spark/work-dir/tests' ]
    networks:
      - data-net


# Dev Testing Spark container setup:
#
#  master:
#    image: ${COMPOSE_PROJECT_NAME}-spark
#    container_name: ${COMPOSE_PROJECT_NAME}.master
#    command: sbin/start-master.sh
#    volumes:
#      - ./apps:/opt/spark/work-dir
#      - ./data:/opt/spark/data
#    environment:
#      - SPARK_NO_DAEMONIZE=true
#
#    networks:
#        - data-net
#
#
#  worker.1:
#    image: ${COMPOSE_PROJECT_NAME}-spark
#    container_name: ${COMPOSE_PROJECT_NAME}.worker.1
#    volumes:
#      - ./apps:/opt/spark/work-dir
#      - ./data:/opt/spark/data
#    depends_on:
#      - master
#    environment:
#      - SPARK_NO_DAEMONIZE=true
#      - "SPARK_MASTER_HOST=master"
#    command: sbin/start-worker.sh
#    networks:
#      - data-net